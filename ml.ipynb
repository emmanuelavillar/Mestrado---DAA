{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74a8547",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import cv \n",
    "import importlib\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.pipeline import Pipeline as PipelineIMB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier, RUSBoostClassifier\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import utils\n",
    "\n",
    "import joblib\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "from tensorflow.keras import callbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a43e90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c4391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dados/pos_analyse.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"Unnamed: 0\", axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d766560",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89477f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035704b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=[\"AVERAGE_SPEED_DIFF_N\"])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa842849",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = pd.crosstab(data[\"LUMINOSITY\"], data[\"AVERAGE_CLOUDINESS\"])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(ct, aspect='auto')\n",
    "plt.colorbar(label='Contagem')\n",
    "\n",
    "plt.xticks(range(len(ct.columns)), ct.columns, rotation=90)\n",
    "plt.yticks(range(len(ct.index)), ct.index)\n",
    "\n",
    "plt.xlabel(\"AVERAGE_CLOUDINESS_N\")\n",
    "plt.ylabel(\"LUMINOSITY_N\")\n",
    "plt.title(\"Relação entre LUMINOSITY e AVERAGE_CLOUDINESS\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeccc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preencher_cloudiness(row):\n",
    "    if pd.notna(row[\"AVERAGE_CLOUDINESS_N\"]):\n",
    "        return row[\"AVERAGE_CLOUDINESS_N\"]\n",
    "    \n",
    "    if row[\"LUMINOSITY_N\"] == 2: # light\n",
    "        return 1   # céu claro\n",
    "    \n",
    "    if row[\"LUMINOSITY_N\"] == 0: # dark\n",
    "        return 7   # nublado\n",
    "    \n",
    "    if row[\"LUMINOSITY_N\"] == 1: #LOW_LIGHT\n",
    "        return 3 # algumas nuvens\n",
    "\n",
    "data[\"AVERAGE_CLOUDINESS_N\"] = data.apply(preencher_cloudiness, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a35f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46520e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"record_date\"] = pd.to_datetime(data[\"record_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by='record_date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c62907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1492cb",
   "metadata": {},
   "source": [
    "### Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5dc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['record_date','AVERAGE_FREE_FLOW_SPEED',\n",
    "       'AVERAGE_TIME_DIFF', 'AVERAGE_FREE_FLOW_TIME',\n",
    "       'AVERAGE_TEMPERATURE',\n",
    "       'hour', 'LUMINOSITY_N',\n",
    "       'is_peak_hour', 'hour_weekday', 'time_ratio']]\n",
    "\n",
    "y = data['AVERAGE_SPEED_DIFF_N']\n",
    "\n",
    "#X = X.dropna()\n",
    "#y = y.loc[X.index]\n",
    "\n",
    "# Time-Based Split \n",
    "# cutoff = data['record_date'].quantile(0.70)  \n",
    "# train_mask = data['record_date'] <= cutoff\n",
    "\n",
    "# X_train, X_test = X[train_mask], X[~train_mask]\n",
    "# y_train, y_test = y[train_mask], y[~train_mask]\n",
    "\n",
    "# #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = n, stratify=y)\n",
    "\n",
    "# print(f\"Datas do treino: {data[train_mask]['record_date'].min()} a {data[train_mask]['record_date'].max()}\")\n",
    "# print(f\"Datas do teste: {data[~train_mask]['record_date'].min()} a {data[~train_mask]['record_date'].max()}\")\n",
    "\n",
    "# Time-Based Split\n",
    "cutoff = data['record_date'].quantile(0.70)\n",
    "train_mask = data['record_date'] <= cutoff\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = n, stratify=y)\n",
    "\n",
    "X_train_full, X_test = X[train_mask], X[~train_mask]\n",
    "y_train_full, y_test = y[train_mask], y[~train_mask]\n",
    "\n",
    "cutoff_val = X_train_full['record_date'].quantile(0.80)  \n",
    "train_mask_inner = X_train_full['record_date'] <= cutoff_val\n",
    "\n",
    "X_train, X_val = X_train_full[train_mask_inner], X_train_full[~train_mask_inner]\n",
    "y_train, y_val = y_train_full[train_mask_inner], y_train_full[~train_mask_inner]\n",
    "\n",
    "#LOOKKK\n",
    "X_train = X_train.drop(columns='record_date')\n",
    "X_val = X_val.drop(columns='record_date')\n",
    "X_test = X_test.drop(columns='record_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22007c",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ad182",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''O CatBoostEncoder substitui valores categóricos por uma versão suavizada da média da variável alvo (target mean encoding), levando em consideração a distribuição da variável alvo para evitar overfitting'''\n",
    "\n",
    "def f_preprocessor ():\n",
    "    cols_num = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cols_cat = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    pipe_cat = Pipeline([('encoder', OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore'))]) #Pipeline([('encoder', CatBoostEncoder())])#\n",
    "    pipe_num = Pipeline([('std_scaler', StandardScaler())])\n",
    "\n",
    "    preprocessor = ColumnTransformer([('numeric', pipe_num, cols_num), #'passthrough'\n",
    "                                    ('categoric', pipe_cat, cols_cat)])\n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = f_preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fd365",
   "metadata": {},
   "source": [
    "### Cross Validation (treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a07c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cv)\n",
    "df_over = cv.cross_validation_models_set_class('Default', preprocessor, X_train, y_train)\n",
    "df_over.sort_values('F1-Score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8921ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cv)\n",
    "df_over = cv.cross_validation_models_set_class('Oversampling', preprocessor, X_train, y_train)\n",
    "df_over.sort_values('F1-Score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d23af",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efff7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n = 42\n",
    "\n",
    "decision_tree_params = {\n",
    "    'model__criterion': ['gini', 'entropy'],  \n",
    "    'model__max_depth': [None, 5, 10, 20], \n",
    "    'model__min_samples_split': [2, 5, 10],  \n",
    "    'model__min_samples_leaf': [1, 2, 4]  \n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'model__n_estimators': [100, 200],  \n",
    "    'model__criterion': ['gini', 'entropy'], \n",
    "    'model__max_depth': [None, 5, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__bootstrap': [True, False] \n",
    "}\n",
    "\n",
    "balanced_random_forest_params = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_depth': [None, 5, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__bootstrap': [True, False],\n",
    "    'model__sampling_strategy': ['auto', 0.5, 0.75],  # Controle do balanceamento\n",
    "    'model__replacement': [True, False],  # Amostragem com/sem reposição\n",
    "    'model__class_weight': [None, 'balanced', 'balanced_subsample']\n",
    "}\n",
    "\n",
    "gradient_boosting_params = {\n",
    "    'model__learning_rate': [0.1, 0.01],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "logistic_regression_params = {\n",
    "    'model__C': [0.1, 1, 10],  # regularização\n",
    "    'model__penalty': ['l1', 'l2'],  # tipo de regularização\n",
    "    'model__solver': ['liblinear']  \n",
    "}\n",
    "\n",
    "# Parâmetros para SVM\n",
    "svm_params = {\n",
    "    'model__C': [0.1, 1, 10],  \n",
    "    'model__kernel': ['linear', 'rbf'],  \n",
    "    'model__gamma': ['scale', 'auto']  # Coeficiente para kernel 'rbf'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "knn_params = {\n",
    "    'model__n_neighbors': [3, 5, 7, 10],  # Número de vizinhos a serem considerados\n",
    "    'model__weights': ['uniform', 'distance'],  # Peso dado aos vizinhos\n",
    "    'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algoritmo usado para encontrar os vizinhos\n",
    "    'model__leaf_size': [20, 30, 40],  # Tamanho das folhas (afeta a eficiência)\n",
    "    'model__p': [1, 2]  # Tipo de distância (1 = Manhattan, 2 = Euclidiana)\n",
    "}\n",
    "\n",
    "\n",
    "# Parâmetros para LightGBM\n",
    "lightgbm_params = {\n",
    "    'model__num_leaves': [31, 50, 100],          # Número máximo de folhas em uma árvore\n",
    "    'model__max_depth': [-1, 5, 10],             # Profundidade máxima (-1 significa sem limite)\n",
    "    'model__learning_rate': [0.1, 0.01, 0.001],  # Taxa de aprendizado\n",
    "    'model__n_estimators': [100, 200],           # Número de árvores\n",
    "    'model__min_child_samples': [20, 50],        # Número mínimo de dados em uma folha\n",
    "    'model__subsample': [0.8, 1.0],              # Fração de amostras para treino\n",
    "    'model__colsample_bytree': [0.8, 1.0],       # Fração de features para construir cada árvore\n",
    "    'model__reg_alpha': [0, 0.1],                # Regularização L1\n",
    "    'model__reg_lambda': [0, 0.1],               # Regularização L2\n",
    "    'model__boosting_type': ['gbdt', 'dart']      # Tipo de boosting\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'model__max_depth': [3, 5, 7],\n",
    "    'model__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__subsample': [0.8, 1.0],\n",
    "    'model__colsample_bytree': [0.8, 1.0],\n",
    "    'model__gamma': [0, 0.1, 0.2],\n",
    "    'model__reg_alpha': [0, 0.1],\n",
    "    'model__reg_lambda': [1, 0.1],\n",
    "    'model__min_child_weight': [1, 5],\n",
    "    'model__tree_method': ['hist']  # Faster for medium-sized datasets\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'model__depth': [8, 10, 12],\n",
    "    'model__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'model__iterations': [100, 200],\n",
    "    'model__l2_leaf_reg': [3, 5, 7],\n",
    "    'model__border_count': [64, 96],\n",
    "    'model__grow_policy': ['SymmetricTree'],  \n",
    "    'model__auto_class_weights': ['Balanced'], \n",
    "    'model__boosting_type': ['Ordered']\n",
    "}\n",
    "\n",
    "mlp_params = {\n",
    "    'model__hidden_layer_sizes': [(50,), (100,), (100, 50), (100, 100)],\n",
    "    'model__activation': ['relu', 'tanh'],  # 'logistic' também é possível\n",
    "    'model__solver': ['adam'],  \n",
    "    'model__alpha': [0.0001, 0.001, 0.01],  # Regularização L2\n",
    "    'model__learning_rate_init': [0.001, 0.01],  # Taxa de aprendizado inicial\n",
    "    'model__max_iter': [200, 300],\n",
    "    'model__early_stopping': [True],  # Para evitar overfitting\n",
    "    'model__random_state': [42]  # Para reprodutibilidade\n",
    "}\n",
    "\n",
    "easy_ensemble_params = {\n",
    "    'model__n_estimators': [10, 30, 50, 100],\n",
    "    'model__estimator': [DecisionTreeClassifier(max_depth=5)],  # Removed None\n",
    "    'model__warm_start': [True, False],\n",
    "    'model__sampling_strategy': ['auto', 0.5, 0.75, 1.0],\n",
    "    'model__replacement': [True, False],\n",
    "    'model__random_state': [None, 42],\n",
    "    'model__estimator__criterion': ['gini', 'entropy'],\n",
    "    'model__estimator__max_depth': [None, 5, 10],\n",
    "    'model__estimator__min_samples_split': [2, 5, 10],\n",
    "    'model__estimator__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelos = {\n",
    "    #'Gradient Boosting': (GradientBoostingClassifier(random_state=n), gradient_boosting_params),\n",
    "    #'Regressão Logística': (LogisticRegression(random_state=n, max_iter=1000), logistic_regression_params),\n",
    "    #'SVM': (SVC(random_state=n, probability=True), svm_params),\n",
    "    # 'KNN': (KNeighborsClassifier(), knn_params),\n",
    "    #'MLP' : (MLPClassifier(),mlp_params),\n",
    "    #'XGBoost': (XGBClassifier(random_state=n), xgb_params),\n",
    "    #'LightGBM': (LGBMClassifier(random_state=n), lightgbm_params)\n",
    "    #'Decision Tree': (DecisionTreeClassifier(random_state=n), decision_tree_params)\n",
    "    'Random Forest': (RandomForestClassifier(random_state=n), random_forest_params),\n",
    "    'CatBoost': (CatBoostClassifier(random_state=n, verbose=0, auto_class_weights='Balanced'), catboost_params),\n",
    "    #'Balanced Random Forest': (BalancedRandomForestClassifier(random_state=n), balanced_random_forest_params),\n",
    "    #'Easy Ensemble': (EasyEnsembleClassifier(random_state=42), easy_ensemble_params)\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec045f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cv)\n",
    "gs = cv.hyperparameter_optimization(None,preprocessor, X_train, y_train, modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c14772",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(cv)\n",
    "gs = cv.hyperparameter_optimization(SMOTE(),preprocessor, X_train, y_train, modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac14bed",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ab5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 42\n",
    "\n",
    "catboost = CatBoostClassifier(\n",
    "    auto_class_weights='Balanced',\n",
    "    depth=3,  \n",
    "    learning_rate=0.005,  \n",
    "    iterations=2000,\n",
    "    border_count=32, \n",
    "    boosting_type='Plain',\n",
    "    grow_policy='SymmetricTree',\n",
    "    l2_leaf_reg=50, \n",
    "    od_type='Iter',\n",
    "    od_wait=15,\n",
    "    random_state=n,\n",
    "    verbose=True,\n",
    "    \n",
    "    min_data_in_leaf=50, \n",
    "    rsm=0.5, \n",
    "    bootstrap_type='Bernoulli',\n",
    "    subsample=0.7, \n",
    "    random_strength=3.0,  \n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "pipe_cb = PipelineIMB([\n",
    "    ('preprocessor', preprocessor),\n",
    "    \n",
    "    ('model', catboost)\n",
    "])\n",
    "\n",
    "\n",
    "randomforest = RandomForestClassifier(bootstrap = False, criterion = 'gini', max_depth =10, min_samples_leaf =1, min_samples_split= 2, n_estimators= 100)\n",
    "pipe_rf = PipelineIMB([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', randomforest)\n",
    "])\n",
    "\n",
    "balancedrandomforest= BalancedRandomForestClassifier(class_weight = None, criterion= 'gini', max_depth =10, min_samples_leaf = 2, min_samples_split= 2, n_estimators =100, replacement =True, sampling_strategy='auto',\n",
    "                                                     bootstrap=True, oob_score=True)\n",
    "pipe_brf =PipelineIMB([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampler', SMOTE()),\n",
    "    ('model', balancedrandomforest)\n",
    "])\n",
    "\n",
    "#### Treinos\n",
    "\n",
    "pipe_cb.fit(X_train, y_train,\n",
    "             model__eval_set=[(X_val, y_val)])\n",
    "#pipe_rf.fit(X_train, y_train)\n",
    "#pipe_brf.fit(X_train, y_train)\n",
    "\n",
    "trained = pipe_cb.named_steps['model']\n",
    "\n",
    "history = trained.evals_result_\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for key in history['learn']:\n",
    "    plt.plot(history['learn'][key], label=f'Train {key}')\n",
    "    \n",
    "for key in history['validation']:\n",
    "    plt.plot(history['validation'][key], label=f'Validation {key}')\n",
    "\n",
    "plt.xlabel('Iteração')\n",
    "plt.ylabel('Valor da Métrica')\n",
    "plt.title('Curva de Aprendizado - CatBoost')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "#print(f\"OOB Score: {pipe_brf.named_steps['model'].oob_score_:.4f}\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80908f5b",
   "metadata": {},
   "source": [
    "##### Rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc69a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor.fit(X_train)\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "input_dim = X_train_processed.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "nn_model = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.05)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(n_classes if n_classes > 2 else 1,\n",
    "                activation='softmax' if n_classes > 2 else 'sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.005),\n",
    "    loss='categorical_crossentropy' if n_classes > 2 else 'binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "def train_and_plot_nn_with_early_stop(X_train, y_train, X_val, y_val, model, \n",
    "                                     epochs=200, batch_size=32, patience=15):\n",
    "\n",
    "    # Preparar labels\n",
    "    if len(np.unique(y_train)) > 2:\n",
    "        y_train_nn = keras.utils.to_categorical(y_train)\n",
    "        y_val_nn = keras.utils.to_categorical(y_val)\n",
    "    else:\n",
    "        y_train_nn = y_train.reshape(-1, 1)\n",
    "        y_val_nn = y_val.reshape(-1, 1)\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stopping = callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=12,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=8,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train_nn,\n",
    "        validation_data=(X_val, y_val_nn),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    \n",
    "    if 'accuracy' in history.history:\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy', linestyle='--')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linestyle='--')\n",
    "    \n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        best_epoch = early_stopping.stopped_epoch - patience\n",
    "        plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.5, label=f'Best Epoch ({best_epoch})')\n",
    "    \n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.title('Curva de Aprendizado - Rede Neural (com Early Stopping)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    if early_stopping.stopped_epoch > 0:\n",
    "        print(f\"\\nEarly stopping at epoch {early_stopping.stopped_epoch}\")\n",
    "        print(f\"Best epoch: {early_stopping.stopped_epoch - patience}\")\n",
    "        print(f\"Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "print(\"Treinando Rede Neural com Early Stopping...\")\n",
    "trained_nn, history_nn = train_and_plot_nn_with_early_stop(\n",
    "    X_train_processed, y_train,\n",
    "    X_val_processed, y_val,\n",
    "    nn_model,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    patience=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c6ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_brf = pipe_brf.predict(X_test)\n",
    "# predictions_p_brf =  pipe_brf.predict_proba(X_test)[:, 1] \n",
    "\n",
    "# predictions_cb = pipe_cb.predict(X_test)\n",
    "# predictions_p_cb =  pipe_cb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "predictions_nn_p = trained_nn.predict(X_test_processed)\n",
    "predictions_nn= predictions_nn_p.argmax(axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d0839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_report = classification_report(y_test, predictions_nn)\n",
    "print(\"Relatório de Classificação RF:\")\n",
    "print(rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "cm = confusion_matrix(y_test, predictions_nn, labels=classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = classes)\n",
    "disp = disp.plot(cmap = plt.cm.BuPu)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.curva_roc(X_test, y_test, pipe_brf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf66c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "#utils.shap_tree(preprocessor, X, X_test, pipe_cb)\n",
    "utils.feature_importance(pipe_brf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159427af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_report = classification_report(y_test, predictions_cb)\n",
    "print(\"Relatório de Classificação CB:\")\n",
    "print(rf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e772a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions_cb, labels=pipe_cb.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = pipe_cb.classes_)\n",
    "disp = disp.plot(cmap = plt.cm.BuPu)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5db6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.curva_roc(X_test, y_test, pipe_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "#utils.shap_tree(preprocessor, X, X_test, pipe_cb)\n",
    "utils.feature_importance(pipe_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53326c8",
   "metadata": {},
   "source": [
    "### Salvar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c0b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_final = PipelineIMB([('preprocessor', preprocessor), \n",
    "                              ('resampler', SMOTE()),\n",
    "                              ('model', balancedrandomforest)])\n",
    "joblib.dump(pipeline_final, 'modelos/balanced_random_forest_c3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_final = PipelineIMB([('preprocessor', preprocessor), \n",
    "                              ('model', catboost)])\n",
    "joblib.dump(pipeline_final, 'modelos/catboost_c3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b13b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nn = PipelineIMB([\n",
    "    ('preprocessor', preprocessor), \n",
    "    ('model', trained_nn)  \n",
    "])\n",
    "\n",
    "joblib.dump(pipeline_nn, 'modelos/rede_neural_c3.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
